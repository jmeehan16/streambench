\section{Implementation}
This section details the implementation of our benchmarking system.
\subsection{Cluster}
As mentioned in section~\ref{ssec:arch-phil}, the ideal cluster for a Spark system involves one master machine, X worker nodes, X tuple generators, and a node to collect output.  Previous benchmarking attempts have been performed on Amazon's EC2 machines.  Through the use of Amazon's infrastructure it provides the advantage of having an elastically-sized cluster that can easily be set up and replicated.  The disadvantage of using EC2 machines, however, is that each node is a virtual machine that may not be receiving the resources that are expected.  Additionally, there is little to no control over the network bandwidth on these machines, or what other information might be affecting throughput.  For these reasons, we chose to build our own cluster instead.  This was later determined to be a mistake (see Section~\ref{sec:lessons}).

Our cluster was limited to four AMD Athlon(tm) 64 X2 Dual Core Processor 3800+ machines, each with 3GB of memory and 2 core, 2Ghz processors.  For our single-node experiments, tuple generation and collection were performed on separate nodes from the actual benchmark in an effort to record more accurate throughput numbers.  However, for Spark clusters of size 2 or more, the tuple generators were run on the same machines as the worker nodes due to a lack of machines.

\subsection{Tuple Generator}
As briefly mentioned in Section~\ref{ssec:tuple-gen-meth} tuples are defined as a timestamp followed by a sentence of variable number of words.  Generating these tuples is a two-fold process.  First, a sample file of sentences of a specified word count is pre-generated using a python script.  By generating this file ahead of time, we are able to normalize the input sent to both Spark and System-X.

A second python script then processes our sample file of sentences.  This script prepends a timestamp to the string as the first word.  The timestamp includes of "TS-" in order to distinguish the timestamp from the other words.  The tuples are then sent to the streaming application at a throughput rate specified as an input parameter.  The throughput rate starts small, and gradually increases until the streaming application is saturated and eventually fails.

Initially, the throughput rate on our tuple generator program was simply controlled using the Thread.sleep command.  However, it was discovered that this method is not granular enough to generate tuples at a high enough rate to consistently saturate our systems.  The reason for this is that Thread.sleep is unreliable beyond approximately a 0.1 ms waittime.  This means that the maximum throughput of our generator was about 10,000 tuples.

Our solution to this problem was to add a second factor, a batch size for
tuples.  To generate a higher throughput, we choose to send a batch of tuples,
then wait for a specified amount of time, then send the next batch, and so on.
The equation used to calculate the optimal batch size and wait time is as follows:

{\large $B = \lfloor\frac{T}{2000}\rfloor - 1$}

We determined what our wait time should be using the following equation:

{\large $W = B * \frac{1000}{T}$}

By plugging the result for $B$ into the first equation, we are able to
calculate the optimal wait time to generate the desired throughput.\jeff{reference what W,B,T are}

\subsection{Performance Analysis}
In our original implementation, none of the performance analysis was performed within the streaming system.  Instead, the Wordcount or Grep output was sent to a separate machine, where it would be written to a file and analyzed by a separate script.  This script would calculate throughput and latency numbers by comparing the timestamps within the sentence tuples to the timestamps attached by the streaming system.  While this method did not require additional processing by the system, we found that the act of printing so much information to the TCP output stream was negatively affecting our performance.  We determined that it would be more fair to instead perform the aggregate within the streaming system itself, and only output this number.

In the Wordcount benchmark, after the words have been counted for a given window, the "TS-" timestamp words are filtered, and their respective counts are summed in order to determine the throughput.  The minimum timestamp from that window is then subtracted from the current timestamp in order to calculate the latency.

In the Grep benchmark, it was impossible to calculate throughput numbers after the grep count (number of sentences with the word "the") had been determined.  This is because some tuples had been filtered, but should be counted towards throughput.  Instead, we simply branch the throughput and latency calculations from the source of the stream, giving us accurate numbers for all input tuples.
